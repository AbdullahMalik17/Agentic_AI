{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahMalik17/Agentic_AI/blob/main/api_basics_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9a97e0",
      "metadata": {
        "id": "4a9a97e0"
      },
      "source": [
        "# APIs ‚Äî Hands‚Äëon with Python\n",
        "This Colab walks you through three live API calls:\n",
        "1. Weather forecast\n",
        "2. Cat facts\n",
        "3. OpenAI Chat Completions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ installation"
      ],
      "metadata": {
        "id": "gcZTDV5CBHrk"
      },
      "id": "gcZTDV5CBHrk"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa969b2b",
      "metadata": {
        "id": "fa969b2b"
      },
      "outputs": [],
      "source": [
        "!pip -q install requests openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Imports"
      ],
      "metadata": {
        "id": "d1CPo0lHIwni"
      },
      "id": "d1CPo0lHIwni"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f5d5582",
      "metadata": {
        "id": "7f5d5582"
      },
      "outputs": [],
      "source": [
        "import requests, pprint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca3b817",
      "metadata": {
        "id": "dca3b817"
      },
      "source": [
        "##  ‚öôÔ∏è 1. Get current weather for London (Simple api call )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.open-meteo.com/v1/forecast?latitude=53.5&longitude=-0.15&current_weather=true\"\n",
        "resp = requests.get(url, timeout=10)\n",
        "resp.raise_for_status()\n",
        "weather = resp.json()\n",
        "pprint.pp(weather)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w36xxy0hI0Lh",
        "outputId": "9010e833-e0e7-429c-d0b5-e9ad385c2e37"
      },
      "id": "w36xxy0hI0Lh",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'latitude': 53.5,\n",
            " 'longitude': -0.1400001,\n",
            " 'generationtime_ms': 0.056743621826171875,\n",
            " 'utc_offset_seconds': 0,\n",
            " 'timezone': 'GMT',\n",
            " 'timezone_abbreviation': 'GMT',\n",
            " 'elevation': 52.0,\n",
            " 'current_weather_units': {'time': 'iso8601',\n",
            "                           'interval': 'seconds',\n",
            "                           'temperature': '¬∞C',\n",
            "                           'windspeed': 'km/h',\n",
            "                           'winddirection': '¬∞',\n",
            "                           'is_day': '',\n",
            "                           'weathercode': 'wmo code'},\n",
            " 'current_weather': {'time': '2025-07-21T05:30',\n",
            "                     'interval': 900,\n",
            "                     'temperature': 15.1,\n",
            "                     'windspeed': 4.0,\n",
            "                     'winddirection': 207,\n",
            "                     'is_day': 1,\n",
            "                     'weathercode': 80}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e226af",
      "metadata": {
        "id": "43e226af"
      },
      "source": [
        "## ‚öôÔ∏è 2. Swap endpoint: Cat Fact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d1a75b24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1a75b24",
        "outputId": "25ce2cfe-4b6e-411c-927b-1d0b7452417a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cats do not think that they are little people. They think that we are big cats. This influences their behavior in many ways.\n"
          ]
        }
      ],
      "source": [
        "url = \"https://catfact.ninja/fact\"\n",
        "resp = requests.get(url, timeout=10)\n",
        "resp.raise_for_status()\n",
        "print(resp.json()['fact'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b531c0bf",
      "metadata": {
        "id": "b531c0bf"
      },
      "source": [
        "## 3. üîê Load API Key\n",
        "Fill in your API key below. You can store it in an environment variable or directly in the code (not recommended for production)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "LzAes2NNygH-"
      },
      "id": "LzAes2NNygH-",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Behavior - Stateless or Stateful"
      ],
      "metadata": {
        "id": "27Ie3wbFBel_"
      },
      "id": "27Ie3wbFBel_"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2d2be137",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "2d2be137",
        "outputId": "5675e5e7-66e3-4e0d-af44-bbea338b1409"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-2033437370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPENAI_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Say hello! I'm Wania\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "import os, openai\n",
        "\n",
        "# üîë TODO: Replace with your own key or set OPENAI_API_KEY in the environment\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say hello! I'm Wania\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What's my name?\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FIPkwn9zd1i",
        "outputId": "6460d498-f61e-49c7-d4bb-ce75e2564f03"
      },
      "id": "1FIPkwn9zd1i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I don't have access to that information. If you'd like, you can tell me your name!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Responses API\n"
      ],
      "metadata": {
        "id": "TTDSEFOEzmmp"
      },
      "id": "TTDSEFOEzmmp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Behavior - Stateless or Stateful"
      ],
      "metadata": {
        "id": "orQ1nnIvBwIe"
      },
      "id": "orQ1nnIvBwIe"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                { \"type\": \"input_text\", \"text\": \"what is in this image?\" },\n",
        "                {\n",
        "                    \"type\": \"input_image\",\n",
        "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output[0].content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvbonFkyzj0A",
        "outputId": "8d3cd87a-d19e-49f0-82e1-992658b55932"
      },
      "id": "QvbonFkyzj0A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image shows a wooden boardwalk path running through a lush, green field or wetland area. The scene appears to be natural and tranquil, with tall grasses and vegetation on either side of the walkway. In the background, there are some trees and bushes. The sky is blue with some clouds, suggesting a pleasant, possibly late afternoon or early evening setting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    store=True,\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                { \"type\": \"input_text\", \"text\": \"hello, I'm Wania\" },\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output[0].content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZyYfvcv0hbL",
        "outputId": "26859a2e-a3c5-478e-a8d8-87f3fd9d96f5"
      },
      "id": "5ZyYfvcv0hbL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Wania! üëã How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    store=True,\n",
        "    previous_response_id=response.id,  # üëà linking to earlier message\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                { \"type\": \"input_text\", \"text\": \"What is my name?\" },\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Follow-up response:\", next_response.output[0].content[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oIgMo_k1xUe",
        "outputId": "5c291c85-7d85-4d16-febb-f3bb3b4938a2"
      },
      "id": "2oIgMo_k1xUe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Follow-up response: Your name is Wania! üòä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üòä Gemini Code"
      ],
      "metadata": {
        "id": "naMFaJHPFvJA"
      },
      "id": "naMFaJHPFvJA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Imports"
      ],
      "metadata": {
        "id": "ZIA6rLuOEhj5"
      },
      "id": "ZIA6rLuOEhj5"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "IwbuyW5wDlJa"
      },
      "id": "IwbuyW5wDlJa",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîê Load API Key"
      ],
      "metadata": {
        "id": "_p4GZAVmDv6Y"
      },
      "id": "_p4GZAVmDv6Y"
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "FpkOZ-w6D2hJ"
      },
      "id": "FpkOZ-w6D2hJ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Initialize Gemini Client"
      ],
      "metadata": {
        "id": "rfJF3kT2EAtZ"
      },
      "id": "rfJF3kT2EAtZ"
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")"
      ],
      "metadata": {
        "id": "W42Ax6PKEDho"
      },
      "id": "W42Ax6PKEDho",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main1():\n",
        "  print(\"Getting Data from AI or LLM\")\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model = \"gemini-2.5-pro\",\n",
        "      messages = [\n",
        "          {\"role\":\"system\",\"content\":\"You are a helpful Assistant\"},\n",
        "          {\"role\":\"user\",\"content\":\"What is the LLM?\"}\n",
        "      ]\n",
        "  )\n",
        "  print(response.choices[0].message.content)\n",
        "main1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsTGfyjOgYlP",
        "outputId": "7c59654c-0a19-4c64-8ba6-4fcf0fccddd2"
      },
      "id": "BsTGfyjOgYlP",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting Data from AI or LLM\n",
            "Of course! This is an excellent question.\n",
            "\n",
            "In the simplest terms, an **LLM (Large Language Model)** is a type of artificial intelligence (AI) designed to understand, generate, and interact with human language. Think of it as an incredibly advanced version of the autocomplete on your phone, but on a massive scale.\n",
            "\n",
            "Let's break down the name:\n",
            "\n",
            "*   **Large:** This refers to the enormous size of both the AI model itself and the data it was trained on.\n",
            "    *   **The Data:** LLMs are trained on a gigantic amount of text and code from the internet, books, articles, and other sources‚Äîwe're talking hundreds of billions of words.\n",
            "    *   **The Model:** The model has billions (or even trillions) of \"parameters,\" which are like knobs that get tuned during training to capture the patterns, grammar, context, and nuances of language.\n",
            "\n",
            "*   **Language:** Its primary focus is human language. It learns the statistical relationships between words. It's not just about what a word *means*, but also about which words are likely to follow other words in any given context.\n",
            "\n",
            "*   **Model:** It's a \"model\" of the language it learned. It's not a database of facts (though it knows many) and it doesn't \"think\" or \"understand\" in the human sense. It's a complex mathematical system for predicting the most plausible next word in a sequence.\n",
            "\n",
            "---\n",
            "\n",
            "### How Does It Work (A Simple Analogy)\n",
            "\n",
            "Imagine you have read every book in the world's largest library. You wouldn't have memorized every single sentence, but you would have an incredible intuition for how language works.\n",
            "\n",
            "If someone started a sentence, \"The first person to walk on the moon was...,\" you would instantly know that the most likely next word is \"Neil.\" You'd follow that with \"Armstrong.\"\n",
            "\n",
            "An LLM does this on a much more complex level. When you give it a prompt (a question or instruction), it calculates the most probable sequence of words to form a coherent and relevant response, one word at a time.\n",
            "\n",
            "---\n",
            "\n",
            "### What Can LLMs Do?\n",
            "\n",
            "Because of this core ability, LLMs are incredibly versatile. They can:\n",
            "\n",
            "*   **Answer Questions:** From simple facts to complex explanations.\n",
            "*   **Write Content:** Draft emails, essays, poems, marketing copy, and even computer code.\n",
            "*   **Summarize:** Condense long articles or documents into key points.\n",
            "*   **Translate:** Convert text from one language to another.\n",
            "*   **Brainstorm:** Act as a creative partner to generate ideas.\n",
            "*   **Carry on a Conversation:** This is the basis for chatbots like the one you're talking to right now.\n",
            "\n",
            "---\n",
            "\n",
            "### Famous Examples of LLMs\n",
            "\n",
            "You have likely heard of or used some of these:\n",
            "\n",
            "*   **GPT Series (from OpenAI):** The models that power **ChatGPT**. GPT-3, GPT-4, and their variants are the most famous examples.\n",
            "*   **Gemini (from Google):** The family of models that I am based on. It powers products like Google's AI chat experience (formerly Bard).\n",
            "*   **Llama (from Meta):** A series of powerful open-source models.\n",
            "*   **Claude (from Anthropic):** A family of models known for its focus on safety and constitutional AI.\n",
            "\n",
            "---\n",
            "\n",
            "### Important Limitations\n",
            "\n",
            "It's crucial to remember that LLMs are not perfect. They have key weaknesses:\n",
            "\n",
            "*   **Hallucinations:** They can confidently make up facts or information that is completely wrong. They are designed to sound plausible, not necessarily to be truthful.\n",
            "*   **Bias:** They are trained on human-written text, so they can inherit and amplify the biases (social, racial, gender, etc.) present in the data.\n",
            "*   **No True Understanding:** They don't have consciousness, beliefs, or intentions. They are sophisticated pattern-matching machines.\n",
            "*   **Knowledge Cutoff:** A model's knowledge is frozen at the point its training data ends. It won't know about very recent events unless it has been specifically updated or given access to live information.\n",
            "\n",
            "In short, an **LLM is a powerful AI tool that processes and generates human-like text by predicting the next most likely word, based on patterns learned from a vast amount of training data.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí¨ Run Basic Chat Completion"
      ],
      "metadata": {
        "id": "9XmnFS-NEGrI"
      },
      "id": "9XmnFS-NEGrI"
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"üß† Asking Gemini a question...\\n\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\",   \"content\": \"Explain how AI works in simple terms.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message.content\n",
        "    print(\"üí° Gemini's Response:\\n\")\n",
        "    print(message)\n",
        "main()"
      ],
      "metadata": {
        "id": "TcM49HaSEJ3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3b4b1a-149b-455a-c526-3f2450eb9219"
      },
      "id": "TcM49HaSEJ3Q",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Asking Gemini a question...\n",
            "\n",
            "üí° Gemini's Response:\n",
            "\n",
            "Imagine you're teaching a computer to recognize the difference between a cat and a dog.\n",
            "\n",
            "Here's how AI essentially works, in simple terms:\n",
            "\n",
            "1.  **Learning from Examples (The \"Training\" Phase):**\n",
            "    *   Instead of being explicitly told \"a cat has pointy ears, whiskers, and says meow,\" you show the computer **thousands or millions of pictures** of cats, and thousands or millions of pictures of dogs.\n",
            "    *   For each picture, you **label it**: \"This is a cat,\" \"This is a dog.\" This collection of labeled pictures is called **data**.\n",
            "    *   The AI, which is a complex computer program, then **looks for patterns** in all this data. It starts to figure out what features are common in cat pictures versus dog pictures (e.g., the shape of the snout, the size of the ears, fur texture). It doesn't \"know\" what a cat or dog *is* like we do, but it learns the *visual characteristics* that differentiate them.\n",
            "    *   It's a bit like a child learning: they see many examples, make some mistakes (\"Is that a dog or a cat, mommy?\"), get corrected, and eventually learn to tell the difference on their own.\n",
            "\n",
            "2.  **Making Decisions (The \"Using\" Phase):**\n",
            "    *   Once the AI has \"learned\" from all that data, you can then show it a **brand new picture** it's never seen before.\n",
            "    *   The AI will then **apply the patterns it learned** to this new picture. Based on what it found during its \"training,\" it will make a **prediction** or a **decision**. For example, it might say, \"Based on the patterns I've seen, this new picture is 95% likely to be a cat.\"\n",
            "\n",
            "**The \"Brain\" Behind It:**\n",
            "\n",
            "The \"brain\" of the AI is often a type of program called a **neural network**. These are inspired by the way our own brains work, with many interconnected \"nodes\" (like brain cells) that process information. They adjust their internal connections and \"weights\" as they learn from the data, constantly refining their ability to recognize patterns.\n",
            "\n",
            "**Why is it powerful?**\n",
            "\n",
            "*   **Scale:** AI can process far more data than any human could, finding subtle patterns that we might miss.\n",
            "*   **Speed:** It can make decisions and predictions incredibly fast.\n",
            "*   **Automation:** Once trained, it can perform tasks repeatedly without getting tired or bored.\n",
            "\n",
            "So, in essence, AI works by **feeding it lots of examples (data)**, letting it **find hidden patterns** in that data, and then **using those patterns to make smart decisions or predictions** on new, unseen information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Entry Point"
      ],
      "metadata": {
        "id": "qx0Lsqj5EUdg"
      },
      "id": "qx0Lsqj5EUdg"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6PQQa55EOAZ",
        "outputId": "9f5df7a1-3956-42e8-eeaf-01022f791b24"
      },
      "id": "-6PQQa55EOAZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Asking Gemini a question...\n",
            "\n",
            "üí° Gemini's Response:\n",
            "\n",
            "Imagine you want to teach a computer to tell the difference between a cat and a dog.\n",
            "\n",
            "Here's how AI essentially works, simplified:\n",
            "\n",
            "1.  **Show and Tell (Training Data):**\n",
            "    You don't just tell the computer \"This is a cat.\" Instead, you show it *thousands* (or even millions) of pictures of cats, and for each one, you label it \"cat.\" You do the same with dogs, labeling them \"dog.\" This massive collection of labeled examples is called **training data**.\n",
            "\n",
            "2.  **Pattern Recognition (Learning Algorithms):**\n",
            "    The AI isn't like a human brain. It's more like a very powerful pattern-finding machine. It looks at all these pictures and starts to figure out what common features belong to cats (pointy ears, certain eye shapes, whiskers, etc.) and what belongs to dogs (floppy ears, different snout shapes, etc.). It builds its own internal \"rules\" or \"understanding\" based on these patterns. These \"rules\" are developed by **algorithms**, which are just very smart sets of instructions.\n",
            "\n",
            "3.  **Making a Guess (Prediction):**\n",
            "    Once the AI has \"learned\" from all the training data, you can show it a *brand-new picture* it's never seen before. Based on the patterns it found during training, it will make a guess: \"I think this is a cat\" or \"I think this is a dog.\" The more data it sees and the better its algorithms, the more accurate its guesses will be.\n",
            "\n",
            "**Think of it like this:**\n",
            "\n",
            "*   **Data:** Is the \"food\" the AI eats. The more diverse and relevant the food, the better it learns.\n",
            "*   **Algorithms:** Are the \"recipes\" or \"instruction manuals\" that tell the AI how to process that food and find patterns.\n",
            "*   **Learning:** Is the process of the AI adjusting its internal \"rules\" based on the data to get better at its task.\n",
            "\n",
            "**In essence, AI is about:**\n",
            "\n",
            "*   **Feeding computers vast amounts of data.**\n",
            "*   **Using clever programs (algorithms) to find patterns and relationships in that data.**\n",
            "*   **Allowing the computer to use those patterns to make predictions, decisions, or perform tasks without being explicitly programmed for every single scenario.**\n",
            "\n",
            "It's not \"thinking\" or \"conscious\" like a human, but it's incredibly good at finding complex patterns and acting on them.\n",
            "\n",
            "**Examples you might encounter:**\n",
            "\n",
            "*   **Siri or Alexa:** Trained on millions of voice recordings to recognize your words and understand commands.\n",
            "*   **Netflix recommendations:** Trained on your viewing history and what similar users watch to suggest new movies.\n",
            "*   **Self-driving cars:** Trained on countless hours of driving data, road signs, and object recognition to navigate.\n",
            "*   **Spam filters:** Trained on examples of spam and legitimate emails to identify unwanted messages.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}